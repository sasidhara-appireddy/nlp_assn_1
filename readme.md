hello to the assignment..

INSTALL GIT YOU MINGWITS

## N-Grams

Testing:-
Change model_path â†’ point to the model you want (e.g., ngram_model_500.pkl, ngram_model_2500.pkl).
Change test_data_path â†’ point to your test dataset CSV.
Run the script.
It prints perplexity + classification metrics.

## Hindi

Training phase

The training dataset is read line by line.

Each text is split into tokens.

Both unigrams (single words) and bigrams (pairs of consecutive words) are extracted.

For each label (category) the text belongs to, a frequency table of n-grams is updated.

Then, counts are normalized into probabilities with Laplace smoothing:

ğ‘ƒ
(
ngram
âˆ£
label
)
=
count(ngram,label)

- ğ›¼
  total(label)
- ğ›¼
  â‹…
  âˆ£
  ğ‘‰
  âˆ£
  P(ngramâˆ£label)=
  total(label)+Î±â‹…âˆ£Vâˆ£
  count(ngram,label)+Î±
  â€‹

where
âˆ£
ğ‘‰
âˆ£
âˆ£Vâˆ£ = vocabulary size,
ğ›¼
=
1.0
Î±=1.0.

Classification phase

When a new text is given, it is tokenized into unigrams+bigrams.

For each category, the model computes the log probability of seeing those n-grams under that category:

log
â¡
ğ‘ƒ
(
text
âˆ£
label
)
=
âˆ‘
ngramÂ inÂ text
log
â¡
ğ‘ƒ
(
ngram
âˆ£
label
)
logP(textâˆ£label)=
ngramÂ inÂ text
âˆ‘
â€‹

logP(ngramâˆ£label)

The category with the highest total log probability is chosen as the prediction.

Evaluation phase

Intrinsic evaluation (Perplexity):
Measures how â€œsurprisedâ€ the model is by the test data.
Lower perplexity = better language modeling.

# Perplexity

ğ‘’
âˆ’
1
ğ‘
âˆ‘
log
â¡
ğ‘ƒ
(
ngram
)
Perplexity=e
âˆ’
N
1
â€‹

âˆ‘logP(ngram)

Extrinsic evaluation (Classification):
Compares predicted labels with true labels to compute:

Accuracy (fraction correct)

Precision (correct positive predictions / all positive predictions)

Recall (correct positive predictions / all true positives)

F1 (harmonic mean of precision & recall)
